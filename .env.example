# CARELINK AI Microservice - Environment Configuration
# Copy this file to .env and configure for your environment

# Application Settings
ENVIRONMENT=development
APP_VERSION=1.0.0
DEBUG=true
LOG_LEVEL=INFO

# Server Configuration
HOST=0.0.0.0
PORT=8000

# CORS Settings
CORS_ORIGINS=http://localhost:3000,http://localhost:8000

# Model Configuration
MODEL_PATH=app/models/risk_model.pkl
MODEL_VERSION=v1.0.0

# NLP Configuration
SPACY_MODEL=en_core_web_sm

# LLM Configuration (Ollama - llama3.2 on GPU)
LLM_MODEL_NAME=llama3.2
LLM_DEVICE=cuda
LLM_MAX_LENGTH=512
LLM_TEMPERATURE=0.7
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TIMEOUT=60

# Security
API_KEY_ENABLED=false
API_KEY=your-secret-api-key-here

# Database (SQLite)
DATABASE_URL=sqlite:///./carelink.db

# Monitoring
ENABLE_METRICS=true
ENABLE_AUDIT_LOGGING=true

# Feature Flags
SAFETY_RULES_ENABLED=true
CONFIDENCE_CONTROLLER_ENABLED=true
EXPLANATION_SERVICE_ENABLED=true

# Thresholds
CONFIDENCE_THRESHOLD=0.6
ESCALATION_THRESHOLD=0.75
EMERGENCY_FLAG_THRESHOLD=0.75
